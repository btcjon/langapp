




How to add memory to a Multi-Input Chain | ü¶úÔ∏èüîó Langchain





Skip to main contentü¶úÔ∏èüîó LangChainJS/TS DocsGitHubCTRLKGet startedIntroductionInstallationQuickstartModulesModel I/‚ÄãOData connectionChainsMemoryHow-toHow to add Memory to an LLMChainHow to add memory to a Multi-Input ChainHow to add Memory to an AgentAdding Message Memory backed by a database to an AgentConversation buffer memoryConversation buffer window memoryHow to customize conversational memoryHow to create a custom Memory classEntity memoryConversation Knowledge Graph MemoryHow to use multiple memory classes in the same chainConversation summary memoryConversationSummaryBufferMemoryConversationTokenBufferMemoryVector store-backed memoryIntegrationsAgentsCallbacksModulesUse casesGuidesEcosystemAdditional resourcesAPI referenceModulesMemoryHow-toHow to add memory to a Multi-Input ChainHow to add memory to a Multi-Input ChainMost memory objects assume a single input. In this notebook, we go over how to add memory to a chain that has multiple inputs. As an example of such a chain, we will add memory to a question/answering chain. This chain takes as inputs both related documents and a user question.from langchain.embeddings.openai import OpenAIEmbeddingsfrom langchain.embeddings.cohere import CohereEmbeddingsfrom langchain.text_splitter import CharacterTextSplitterfrom langchain.vectorstores.elastic_vector_search import ElasticVectorSearchfrom langchain.vectorstores import Chromafrom langchain.docstore.document import Documentwith open("../../state_of_the_union.txt") as f:    state_of_the_union = f.read()text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)texts = text_splitter.split_text(state_of_the_union)embeddings = OpenAIEmbeddings()docsearch = Chroma.from_texts(    texts, embeddings, metadatas=[{"source": i} for i in range(len(texts))])    Running Chroma using direct local API.    Using DuckDB in-memory for database. Data will be transient.query = "What did the president say about Justice Breyer"docs = docsearch.similarity_search(query)from langchain.chains.question_answering import load_qa_chainfrom langchain.llms import OpenAIfrom langchain.prompts import PromptTemplatefrom langchain.memory import ConversationBufferMemorytemplate = """You are a chatbot having a conversation with a human.Given the following extracted parts of a long document and a question, create a final answer.{context}{chat_history}Human: {human_input}Chatbot:"""prompt = PromptTemplate(    input_variables=["chat_history", "human_input", "context"], template=template)memory = ConversationBufferMemory(memory_key="chat_history", input_key="human_input")chain = load_qa_chain(    OpenAI(temperature=0), chain_type="stuff", memory=memory, prompt=prompt)query = "What did the president say about Justice Breyer"chain({"input_documents": docs, "human_input": query}, return_only_outputs=True)    {'output_text': ' Tonight, I‚Äôd like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer‚Äîan Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.'}print(chain.memory.buffer)        Human: What did the president say about Justice Breyer    AI:  Tonight, I‚Äôd like to honor someone who has dedicated his life to serve this country: Justice Stephen Breyer‚Äîan Army veteran, Constitutional scholar, and retiring Justice of the United States Supreme Court. Justice Breyer, thank you for your service.PreviousHow to add Memory to an LLMChainNextHow to add Memory to an AgentCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc.




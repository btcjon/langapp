




AutoGPT | ü¶úÔ∏èüîó Langchain





Skip to main contentü¶úÔ∏èüîó LangChainJS/TS DocsGitHubCTRLKGet startedIntroductionInstallationQuickstartModulesModel I/‚ÄãOData connectionChainsMemoryAgentsCallbacksModulesUse casesAgent simulationsAgentsInteracting with APIsAutonomous (long-running) agentsAutoGPTBabyAGI User GuideBabyAGI with Toolsmarathon_timesMeta-PromptChatbotsCode UnderstandingExtractionMulti-modalQA and Chat over DocumentsSummarizationAnalyzing structured dataGuidesEcosystemAdditional resourcesAPI referenceUse casesAutonomous (long-running) agentsAutoGPTOn this pageAutoGPTImplementation of https://github.com/Significant-Gravitas/Auto-GPT but with LangChain primitives (LLMs, PromptTemplates, VectorStores, Embeddings, Tools)Set up tools‚ÄãWe'll set up an AutoGPT with a search tool, and write-file tool, and a read-file toolfrom langchain.utilities import SerpAPIWrapperfrom langchain.agents import Toolfrom langchain.tools.file_management.write import WriteFileToolfrom langchain.tools.file_management.read import ReadFileToolsearch = SerpAPIWrapper()tools = [    Tool(        name="search",        func=search.run,        description="useful for when you need to answer questions about current events. You should ask targeted questions",    ),    WriteFileTool(),    ReadFileTool(),]Set up memory‚ÄãThe memory here is used for the agents intermediate stepsfrom langchain.vectorstores import FAISSfrom langchain.docstore import InMemoryDocstorefrom langchain.embeddings import OpenAIEmbeddings# Define your embedding modelembeddings_model = OpenAIEmbeddings()# Initialize the vectorstore as emptyimport faissembedding_size = 1536index = faiss.IndexFlatL2(embedding_size)vectorstore = FAISS(embeddings_model.embed_query, index, InMemoryDocstore({}), {})Setup model and AutoGPT‚ÄãInitialize everything! We will use ChatOpenAI modelfrom langchain.experimental import AutoGPTfrom langchain.chat_models import ChatOpenAIagent = AutoGPT.from_llm_and_tools(    ai_name="Tom",    ai_role="Assistant",    tools=tools,    llm=ChatOpenAI(temperature=0),    memory=vectorstore.as_retriever(),)# Set verbose to be trueagent.chain.verbose = TrueRun an example‚ÄãHere we will make it write a weather report for SFagent.run(["write a weather report for SF today"])Chat History Memory‚ÄãIn addition to the memory that holds the agent immediate steps, we also have a chat history memory. By default, the agent will use 'ChatMessageHistory' and it can be changed. This is useful when you want to use a different type of memory for example 'FileChatHistoryMemory'from langchain.memory.chat_message_histories import FileChatMessageHistoryagent = AutoGPT.from_llm_and_tools(    ai_name="Tom",    ai_role="Assistant",    tools=tools,    llm=ChatOpenAI(temperature=0),    memory=vectorstore.as_retriever(),    chat_history_memory=FileChatMessageHistory("chat_history.txt"),)PreviousAutonomous (long-running) agentsNextBabyAGI User GuideSet up toolsSet up memorySetup model and AutoGPTRun an exampleChat History MemoryCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc.




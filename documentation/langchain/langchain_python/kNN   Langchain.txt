




kNN | ğŸ¦œï¸ğŸ”— Langchain





Skip to main contentğŸ¦œï¸ğŸ”— LangChainJS/TS DocsGitHubCTRLKGet startedIntroductionInstallationQuickstartModulesModel I/â€‹OData connectionDocument loadersDocument transformersText embedding modelsVector storesRetrieversHow-toIntegrationsAmazon KendraArxivAzure Cognitive SearchChaindeskChatGPT PluginCohere RerankerDocArray RetrieverElasticSearch BM25kNNLOTR (Merger Retriever)MetalPinecone Hybrid SearchPubMedSVMTF-IDFVespaWeaviate Hybrid SearchWikipediaZepChainsMemoryAgentsCallbacksModulesUse casesGuidesEcosystemAdditional resourcesAPI referenceModulesData connectionRetrieversIntegrationskNNOn this pagekNNIn statistics, the k-nearest neighbors algorithm (k-NN) is a non-parametric supervised learning method first developed by Evelyn Fix and Joseph Hodges in 1951, and later expanded by Thomas Cover. It is used for classification and regression.This notebook goes over how to use a retriever that under the hood uses an kNN.Largely based on https://github.com/karpathy/randomfun/blob/master/knn_vs_svm.htmlfrom langchain.retrievers import KNNRetrieverfrom langchain.embeddings import OpenAIEmbeddingsCreate New Retriever with Textsâ€‹retriever = KNNRetriever.from_texts(    ["foo", "bar", "world", "hello", "foo bar"], OpenAIEmbeddings())Use Retrieverâ€‹We can now use the retriever!result = retriever.get_relevant_documents("foo")result    [Document(page_content='foo', metadata={}),     Document(page_content='foo bar', metadata={}),     Document(page_content='hello', metadata={}),     Document(page_content='bar', metadata={})]PreviousElasticSearch BM25NextLOTR (Merger Retriever)Create New Retriever with TextsUse RetrieverCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright Â© 2023 LangChain, Inc.









OpenLLM | ü¶úÔ∏èüîó Langchain





Skip to main contentü¶úÔ∏èüîó LangChainJS/TS DocsGitHubCTRLKGet startedIntroductionInstallationQuickstartModulesModel I/‚ÄãOData connectionChainsMemoryAgentsCallbacksModulesUse casesGuidesEcosystemIntegrationsWandB TracingAI21 LabsAimAirbyteAirtableAleph AlphaAlibaba Cloud OpensearchAmazon API GatewayAnalyticDBAnnoyAnyscaleApifyArgillaArthurArxivAtlasDBAwaDBAWS S3 DirectoryAZLyricsAzure Blob StorageAzure Cognitive SearchAzure OpenAIBananaBasetenBeamBedrockBiliBiliBlackboardBrave SearchCassandraCerebriumAIChaindeskChromaClarifaiClearMLCnosDBCohereCollege ConfidentialCometConfluenceC TransformersDatabricksDatadog LogsDataForSEODeepInfraDeep LakeDiffbotDiscordDocugamiDuckDBElasticsearchEverNoteFacebook ChatFigmaFlyteForefrontAIGitGitBookGoogle BigQueryGoogle Cloud StorageGoogle DriveGoogle SearchGoogle SerperGooseAIGPT4AllGraphsignalGrobidGutenbergHacker NewsHazy ResearchHeliconeHologresHugging FaceiFixitIMSDbInfinoJinaLanceDBLangChain Decorators ‚ú®Llama.cppMarqoMediaWikiDumpMetalMicrosoft OneDriveMicrosoft PowerPointMicrosoft WordMilvusMLflowModalModelScopeModern TreasuryMomentoMotherduckMyScaleNLPCloudNotion DBObsidianOpenAIOpenLLMOpenSearchOpenWeatherMapPetalsPGVectorPineconePipelineAIPrediction GuardPromptLayerPsychicQdrantRay ServeRebuffRedditRedisReplicateRoamRocksetRunhouseRWKV-4SageMaker EndpointSearxNG Search APISerpAPIShale ProtocolSingleStoreDBscikit-learnSlackspaCySpreedlyStarRocksStochasticAIStripeTairTelegramTigris2MarkdownTrelloTruLensTwitterTypesenseUnstructuredVectaraVespaWeights & BiasesWeatherWeaviateWhatsAppWhyLabsWikipediaWolfram AlphaWriterYeager.aiYouTubeZepZillizDependentsAdditional resourcesAPI referenceEcosystemIntegrationsOpenLLMOn this pageOpenLLMThis page demonstrates how to use OpenLLM
with LangChain.OpenLLM is an open platform for operating large language models (LLMs) in
production. It enables developers to easily run inference with any open-source
LLMs, deploy to the cloud or on-premises, and build powerful AI apps.Installation and Setup‚ÄãInstall the OpenLLM package via PyPI:pip install openllmLLM‚ÄãOpenLLM supports a wide range of open-source LLMs as well as serving users' own
fine-tuned LLMs. Use openllm model command to see all available models that
are pre-optimized for OpenLLM.Wrappers‚ÄãThere is a OpenLLM Wrapper which supports loading LLM in-process or accessing a
remote OpenLLM server:from langchain.llms import OpenLLMWrapper for OpenLLM server‚ÄãThis wrapper supports connecting to an OpenLLM server via HTTP or gRPC. The
OpenLLM server can run either locally or on the cloud.To try it out locally, start an OpenLLM server:openllm start flan-t5Wrapper usage:from langchain.llms import OpenLLMllm = OpenLLM(server_url='http://localhost:3000')llm("What is the difference between a duck and a goose? And why there are so many Goose in Canada?")Wrapper for Local Inference‚ÄãYou can also use the OpenLLM wrapper to load LLM in current Python process for
running inference.from langchain.llms import OpenLLMllm = OpenLLM(model_name="dolly-v2", model_id='databricks/dolly-v2-7b')llm("What is the difference between a duck and a goose? And why there are so many Goose in Canada?")Usage‚ÄãFor a more detailed walkthrough of the OpenLLM Wrapper, see the
example notebookPreviousOpenAINextOpenSearchInstallation and SetupLLMWrappersWrapper for OpenLLM serverWrapper for Local InferenceUsageCommunityDiscordTwitterGitHubPythonJS/TSMoreHomepageBlogCopyright ¬© 2023 LangChain, Inc.



